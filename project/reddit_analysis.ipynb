{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "other-shoulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, timezone\n",
    "import re\n",
    "from pyspark.sql.functions import col, udf, size\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from operator import add\n",
    "\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://192.168.2.242:7077\") \\\n",
    "        .appName(\"reddit_analysis_t11\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", True)\\\n",
    "        .config(\"spark.eventLog.enabled\", True)\\\n",
    "        .config(\"spark.eventLog.dir\", \"hdfs://192.168.2.242:9000/user/shared/spark-logs\")\\\n",
    "        .config(\"spark.cores.max\", 8)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Old API (RDD) \n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "spark_context.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "colonial-modeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cores = 10\n",
    "df = spark_session.read.json(\"hdfs://192.168.2.242:9000/user/reddit_data/big/strong_scaling/*\")\n",
    "\n",
    "print(\"Input Sample:\\n\\n\", df.take(2))\n",
    "\n",
    "print(\"\\nNumber of partitions:\", df.rdd.getNumPartitions())\n",
    "\n",
    "print(\"\\nReddit post schema:\\n\")\n",
    "df.printSchema()\n",
    "\n",
    "# Print time stamp of first and last reddit post in this data frame\n",
    "first_timestamp = datetime.fromtimestamp(df.first()[\"created_utc\"], tz=timezone.utc)\n",
    "last_timestamp = datetime.fromtimestamp(df.tail(1)[0][\"created_utc\"], tz=timezone.utc)\n",
    "\n",
    "print(\"First Timestamp of this batch: \", first_timestamp)\n",
    "print(\"Last Timestamp of this batch\", last_timestamp)\n",
    "\n",
    "# currently set the dates encompassing the entire data\n",
    "# can be adjusted to filter on smaller time frames\n",
    "start_ts = int(datetime(2000, 1, 1, 0, 0).timestamp())\n",
    "end_ts = int(datetime(2030, 1, 31, 0, 0).timestamp())\n",
    "\n",
    "# Only interested in these two columnns\n",
    "df = df[['created_utc', 'body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sexual-selection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[created_utc: bigint, body: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.filter((end_ts >= df.created_utc) & (df.created_utc >= start_ts))\n",
    "df = df.filter(df.body != '[deleted]')\n",
    "df = df.filter(df.body != '[removed]')\n",
    "# cache for faster, future computation\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "detailed-scanner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(string):\n",
    "    \"\"\" \n",
    "    UDF that lower cases input string and \n",
    "    removes special characters\n",
    "    \n",
    "    string -- the string to normalize\n",
    "    \"\"\"\n",
    "    return  re.sub(r'[^A-Za-z ]', '', string.lower())\n",
    "\n",
    "udf_normalize = udf(normalize, StringType())\n",
    "\n",
    "df = df.withColumn('body', udf_normalize(col('body')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-outline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the names to filter on \n",
    "pres_cand = [\"donald trump\", \"hillary clinton\"]\n",
    "\n",
    "def return_candidate(string):\n",
    "    \"\"\"\n",
    "    UDF that returns the string representation of a list\n",
    "    of the candidate names from pres_cand that are present in \n",
    "    the input string\n",
    "    \n",
    "    string -- the string to look for candidates in\n",
    "    \"\"\"\n",
    "    present = [cand for cand in pres_cand if cand in string]\n",
    "    if present == []:\n",
    "        present = ''\n",
    "    return str(present)\n",
    "\n",
    "udf_rc = udf(return_candidate, StringType())\n",
    "df = df.withColumn('body', udf_rc(col('body')))\n",
    "df = df.filter(df.body != '')\n",
    "df = df.groupBy('body').count()\n",
    "df.repartition(1).write.csv('save.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-crowd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-maintenance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
